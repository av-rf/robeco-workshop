# -*- coding: utf-8 -*-
"""RF_MSFT_grid_search.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1j8q5K7QcXxyO3wc1oAxtD5GaIZPsCJZO
"""

import pandas as pd
import numpy as py
import matplotlib.pyplot as plt
import tensorflow as tf

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import LSTM
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error

!pip install arch
from arch import arch_model

tf.random.set_seed(777)

# Read csv files and split into 5 dataframes based on stock

df = pd.read_csv("/content/drive/MyDrive/stock_data_train.csv")
df.head(7)

df_MSFT = df.iloc[::5]
df_GE = df.iloc[1::5]
df_AAPL = df.iloc[2::5]
df_BA = df.iloc[3::5]
df_JNJ = df.iloc[4::5]

# scaler = MinMaxScaler(feature_range=(0,1000))

# MSFT
df_MSFT.head(2)
df_MSFT.set_index("date", inplace=True)
df_MSFT = df_MSFT.drop("ticker", axis=1)
# df_MSFT = pd.DataFrame(scaler.fit_transform(df_MSFT), columns = df_MSFT.columns, index = df_MSFT.index)
print(df_MSFT)

# take log? df_MSFT['rv_lead_1'] = py.log(df_MSFT['rv_lead_1'])
plt.figure(figsize=(16,6))
df_MSFT['rv_lead_1'].plot()
plt.title("Rolling Volatility With 30 Time Periods By Annualized Standard Deviation")
plt.show()

# RANDOM FOREST & DECISION TREE

from sklearn.tree import DecisionTreeRegressor as DTR
from sklearn.ensemble import RandomForestRegressor as RFR
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV
# from sklearn.metrics import mean_squared_error

split_index = int(len(df_MSFT) * 0.8)
scaler = StandardScaler()

train_MSFT = df_MSFT[:split_index]
y_train_MSFT = train_MSFT["rv_lead_1"]
X_train_MSFT = scaler.fit_transform(train_MSFT.iloc[:,1:])

test_MSFT = df_MSFT[split_index:]
y_test_MSFT = test_MSFT["rv_lead_1"]
X_test_MSFT = scaler.fit_transform(test_MSFT.iloc[:,1:])

# DECISION TREE
model_DT = DTR(max_depth=10)
model_DT = model_DT.fit(X_train_MSFT, y_train_MSFT)
y_hat_DT_MSFT = model_DT.predict(X_test_MSFT)
mse_DT = mean_squared_error(y_test_MSFT, y_hat_DT_MSFT)

print(f"Mean Squared Error (DT): {mse_DT}")

# RANDOM FOREST
param_grid = {
    'n_estimators': [100, 300, 500],      # Number of trees in the forest
    'max_depth': [10, 20, 30],                 # Maximum depth of the trees
    'min_samples_split': [2, 5, 10],                 # Minimum number of samples required to split a node
    'max_features': ['sqrt', 'log2', None],       # Number of features to consider for the best split
    'bootstrap': [True, False]                       # Whether to use bootstrap samples or not
}

# Initialize the Random Forest model
model_RF = RFR(random_state=42)

# Set up GridSearchCV
grid_search = GridSearchCV(estimator=model_RF,
                           param_grid=param_grid,
                           cv=5,
                           n_jobs=-1,
                           verbose=1,
                           scoring='neg_mean_squared_error',
                           error_score='raise')  # Raise errors if any fit fails

# Fit GridSearchCV to the data
grid_search.fit(X_train_MSFT, y_train_MSFT)

# Print the best parameters and the best score (MSE)
print(f"Best Parameters: {grid_search.best_params_}")
print(f"Best Mean Squared Error (MSE): {-grid_search.best_score_}")

